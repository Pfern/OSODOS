
Reproducibility is a basic requirement of any scientific endeavour. An experiment is simply invalid if, when replicated, 
does not produce the same set of results or least an approximate set of results. Anybody, in the same conditions, should 
be able to follow specifications and reproduce experiments and results. Likewise, experiments shound be robust and perform
 equally well, independently of the observer.

Currently (2016-2017) there is a declared reproducibility crisis. In the biomedical area, attempts to reproduce experiments 
with cancer cells, for example, have repostedly failed. In consequence, some papers had to be retracted. Several efforts have
 been put in place to provide systematic reproduction of experiments at various scales at the level of core facilities, 
laboratories, research institutes, universities and service providers. Since 2012, an international initiative is in place 
to make it happen in a systematic way http://blogs.plos.org/everyone/2012/08/14/plos-one-launches-reproducibility-initiative/

The end result is that scientists became much more concerned about reproducibilty and tightened their controls. Scientific 
societies have studied ways of figthing the lack of reproducibility and issued recommendations (see, for example, 
https://www.asm.org/index.php/colloquium-reports/item/4510-promoting-responsible-scientific-research). Within the 
recommendation, training is clearly a priority, at various levels.

In the scope of this course, the concerns are naturally focused on data. Reproducibily while handligg data is probably easier 
to deal with. In a laboratory that produces measurements one needs to deal with instruments that exibit drifts and require 
a variety of standardisation interventions that we generically call calibratons and controls. A good laboratory service kkeps a 
flawless track of those, and becomes capable of responding to auditing operations at any time. The whole procedure is often 
called quality assurance (QA). Industrial production has needed it, in many aspects, ahead of the research world. It has 
requested very large contributions from statisticians and enginners. Quality assurance is strongly related to qualilty control 
(QC) but is not quite the same: QC refers to the detection and evaluation of faults or disturbances, while QA refers to the 
planning and execution of measures to prevent failures and to respond to their consequences. In many cases it relies on reports 
from QC. This why one often finds QA and QC together in the same process (QA/QC).

Standardised QA/QC procedures allow for **intercalibration**, a generic way of referrring to experiments performed in 
reproducible circumstances in different laboratories or facilities. This is a common way of guaranteeing that quality is not 
assessed differently, therefore facilities can rely on quality to the point of being able to replace each other if needed, when 
for example there is an imbalance in measurement capacity that can be occasionally used to correct overloads of requests.

People concerned with data quality can find a lot of support from the accumulated knowledge and methodological developments in 
this field. Using QA/QC procedures to monitor data quality widens the comfort zone for the researcher, who needs to be concerned 
with the way in which experiments are planned, samples are collected and grouped, etc. Deciding on which (experimental and 
technical) and how many replicates are needed requires statistical skills that are very often below the required level.  

Here are two tutorials that can be helpful:

https://www.moresteam.com/toolbox/design-of-experiments.cfm

https://www.asm.org/index.php/colloquium-reports/item/4510-promoting-responsible-scientific-research



 
